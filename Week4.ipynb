{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/memelordmaddy/WiDS-2023/blob/main/Week4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doyJywopNptX"
      },
      "source": [
        "importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lirRJy_pKze_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import ToTensor, Resize\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y38doxVeXdbt"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Replace the following path with the path to your zipped file on Google Drive\n",
        "zip_path = '/content/gdrive/My Drive/brain_images_dataset.zip'\n",
        "\n",
        "# Define the extraction path\n",
        "extracted_path = '/content/dataset'\n",
        "\n",
        "# Unzip the file\n",
        "!unzip \"$zip_path\" -d \"$extracted_path\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h2S3jFOXZnh"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Resize, ToTensor, Normalize, RandomHorizontalFlip, RandomVerticalFlip, RandomRotation, ColorJitter\n",
        "# Define the transformation to apply to the images\n",
        "transform = transforms.Compose([\n",
        "    Resize((240, 240)),  # Resize the image to 240x240 pixels\n",
        "    transforms.RandomHorizontalFlip(),  # Random horizontal flip for data augmentation\n",
        "    transforms.RandomVerticalFlip(),    # Random vertical flip for data augmentation\n",
        "    transforms.RandomRotation(20),      # Random rotation for data augmentation\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Color jitter\n",
        "    transforms.ToTensor(),              # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to mean=0.5 and std=0.5\n",
        "])\n",
        "\n",
        "# Load the dataset using ImageFolder\n",
        "dataset = ImageFolder(root=extracted_path, transform=transform)\n",
        "\n",
        "# Define the DataLoader to handle batching and shuffling\n",
        "batch_size = 32\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVcuzBdiO00M"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "# Define the size of the training and testing sets\n",
        "train_size = int(0.8 * len(dataset))  # 80% for training\n",
        "test_size = len(dataset) - train_size   # 20% for testing\n",
        "\n",
        "# Use random_split to create the training and testing datasets\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Define the DataLoader for training set\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define the DataLoader for testing set\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H70-9vfjOjlu"
      },
      "outputs": [],
      "source": [
        "# Define the CNN model\n",
        "class BrainTumorCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BrainTumorCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(115200, 256)  # Adjusted the input size based on the output of conv3\n",
        "        self.fc2 = nn.Linear(256, 1)\n",
        "        self.relu = nn.ReLU()  # Added ReLU activation module\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool3(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        logits = self.fc2(x)  # logits for BCEWithLogitsLoss\n",
        "        return logits\n",
        "\n",
        "# Instantiate the model and move it to the GPU if available\n",
        "model = BrainTumorCNN()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()  # Binary Cross Entropy with Logits Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NY5jURDYicvy"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17rWFpoCQfAL"
      },
      "outputs": [],
      "source": [
        "# Define early stopping and model checkpointing\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, delta=0, checkpoint_path='checkpoint.pt'):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.checkpoint_path = checkpoint_path\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = val_loss\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif val_loss > self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = val_loss\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        torch.save(model.state_dict(), self.checkpoint_path)\n",
        "\n",
        "# Instantiate EarlyStopping class\n",
        "early_stopping = EarlyStopping(patience=5, delta=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVPGpMkhU9ec",
        "outputId": "de35e2bb-572e-4a8f-abaf-d6930d540391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.9126, Val Loss: 0.5674, Accuracy: 18.63%\n",
            "Epoch [2/20], Loss: 0.1875, Val Loss: 0.7531, Accuracy: 18.63%\n",
            "Epoch [3/20], Loss: -0.5454, Val Loss: 0.8684, Accuracy: 19.61%\n",
            "Epoch [4/20], Loss: 0.2457, Val Loss: 1.4205, Accuracy: 17.65%\n",
            "Epoch [5/20], Loss: -0.2894, Val Loss: 0.9258, Accuracy: 18.63%\n",
            "Epoch [6/20], Loss: 0.0404, Val Loss: 1.4251, Accuracy: 17.65%\n",
            "Early stopping\n",
            "Test Accuracy: 18.63%\n"
          ]
        }
      ],
      "source": [
        "# Training loop with early stopping and model checkpointing\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.float().view(-1, 1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_loss += criterion(outputs, labels.float().view(-1, 1)).item()\n",
        "\n",
        "            predicted = torch.round(torch.sigmoid(outputs))\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels.view(-1, 1)).sum().item()\n",
        "\n",
        "    val_loss /= len(test_loader)\n",
        "    accuracy = correct / total * 100\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "    # Check early stopping criteria\n",
        "    early_stopping(val_loss, model)\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break\n",
        "\n",
        "# Load the best model weights\n",
        "model.load_state_dict(torch.load(early_stopping.checkpoint_path))\n",
        "\n",
        "# Testing the model\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predicted = torch.round(torch.sigmoid(outputs))\n",
        "        test_total += labels.size(0)\n",
        "        test_correct += (predicted == labels.view(-1, 1)).sum().item()\n",
        "\n",
        "test_accuracy = test_correct / test_total * 100\n",
        "print(f'Test Accuracy: {test_accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "6ygD9KBLbGep",
        "outputId": "6b9d9b7f-39e3-46f9-c3f1-489ddb87d17a"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "index 69 is out of bounds for dimension 0 with size 6",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-1b7ec998ff4e>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Predicted: {prediction}, Actual: {label}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 69 is out of bounds for dimension 0 with size 6"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1500x300 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Assuming 'test_loader' is your DataLoader for the test set\n",
        "# and 'model' is your trained model\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Iterate through the test set and make predictions\n",
        "predictions = []\n",
        "labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, true_labels in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        true_labels = true_labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        predicted_labels = sigmoid(outputs) > 0.5\n",
        "\n",
        "        predictions.extend(predicted_labels.cpu().numpy())\n",
        "        labels.extend(true_labels.cpu().numpy())\n",
        "\n",
        "# Convert predictions and labels to numpy arrays\n",
        "predictions = np.array(predictions)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Visualize some results\n",
        "num_samples = min(5, len(predictions), len(test_loader.dataset))  # Ensure not to exceed the number of available samples\n",
        "fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n",
        "\n",
        "for i in range(num_samples):\n",
        "    sample_index = np.random.randint(len(predictions))\n",
        "    prediction = predictions[sample_index]\n",
        "    label = labels[sample_index]\n",
        "\n",
        "    image = transforms.ToPILImage()(inputs[sample_index].cpu())\n",
        "    axes[i].imshow(image)\n",
        "    axes[i].set_title(f'Predicted: {prediction}, Actual: {label}')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3GvIxLo/ZfswO0RSrO8cm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}